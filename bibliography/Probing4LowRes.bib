
@inproceedings{vaswani_attention_2017,
	title = {Attention is {All} you {Need}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
	abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
	urldate = {2022-08-13},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
	year = {2017},
	file = {Full Text PDF:/home/katja/snap/zotero-snap/common/Zotero/storage/W2GI4986/Vaswani et al. - 2017 - Attention is All you Need.pdf:application/pdf},
}

@inproceedings{devlin_bert_2019,
	address = {Minneapolis, Minnesota},
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {https://aclanthology.org/N19-1423},
	doi = {10.18653/v1/N19-1423},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2022-08-01},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = jun,
	year = {2019},
	pages = {4171--4186},
	file = {Full Text PDF:/home/katja/snap/zotero-snap/common/Zotero/storage/2WH2HXA2/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf},
}

@inproceedings{nivre_universal_2020,
	address = {Marseille, France},
	title = {Universal {Dependencies} v2: {An} {Evergrowing} {Multilingual} {Treebank} {Collection}},
	isbn = {979-10-95546-34-4},
	shorttitle = {Universal {Dependencies} v2},
	url = {https://aclanthology.org/2020.lrec-1.497},
	abstract = {Universal Dependencies is an open community effort to create cross-linguistically consistent treebank annotation for many languages within a dependency-based lexicalist framework. The annotation consists in a linguistically motivated word segmentation; a morphological layer comprising lemmas, universal part-of-speech tags, and standardized morphological features; and a syntactic layer focusing on syntactic relations between predicates, arguments and modifiers. In this paper, we describe version 2 of the universal guidelines (UD v2), discuss the major changes from UD v1 to UD v2, and give an overview of the currently available treebanks for 90 languages.},
	language = {English},
	urldate = {2022-07-11},
	booktitle = {Proceedings of the 12th {Language} {Resources} and {Evaluation} {Conference}},
	publisher = {European Language Resources Association},
	author = {Nivre, Joakim and de Marneffe, Marie-Catherine and Ginter, Filip and Hajič, Jan and Manning, Christopher D. and Pyysalo, Sampo and Schuster, Sebastian and Tyers, Francis and Zeman, Daniel},
	month = may,
	year = {2020},
	pages = {4034--4043},
	file = {Full Text PDF:/home/katja/snap/zotero-snap/common/Zotero/storage/NAJSYRZ4/Nivre et al. - 2020 - Universal Dependencies v2 An Evergrowing Multilin.pdf:application/pdf},
}

@misc{rogers_primer_2020,
	title = {A {Primer} in {BERTology}: {What} we know about how {BERT} works},
	shorttitle = {A {Primer} in {BERTology}},
	url = {http://arxiv.org/abs/2002.12327},
	doi = {10.48550/arXiv.2002.12327},
	abstract = {Transformer-based models have pushed state of the art in many areas of NLP, but our understanding of what is behind their success is still limited. This paper is the first survey of over 150 studies of the popular BERT model. We review the current state of knowledge about how BERT works, what kind of information it learns and how it is represented, common modifications to its training objectives and architecture, the overparameterization issue and approaches to compression. We then outline directions for future research.},
	urldate = {2023-05-31},
	publisher = {arXiv},
	author = {Rogers, Anna and Kovaleva, Olga and Rumshisky, Anna},
	month = nov,
	year = {2020},
	note = {arXiv:2002.12327 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/katja/snap/zotero-snap/common/Zotero/storage/4Z6D7IFJ/Rogers et al. - 2020 - A Primer in BERTology What we know about how BERT.pdf:application/pdf;arXiv.org Snapshot:/home/katja/snap/zotero-snap/common/Zotero/storage/BRND4ITS/2002.html:text/html},
}

@inproceedings{conneau_what_2018,
	address = {Melbourne, Australia},
	title = {What you can cram into a single \$\&!\#* vector: {Probing} sentence embeddings for linguistic properties},
	shorttitle = {What you can cram into a single \$\&!\#* vector},
	url = {https://aclanthology.org/P18-1198},
	doi = {10.18653/v1/P18-1198},
	abstract = {Although much effort has recently been devoted to training high-quality sentence embeddings, we still have a poor understanding of what they are capturing. “Downstream” tasks, often based on sentence classification, are commonly used to evaluate the quality of sentence representations. The complexity of the tasks makes it however difficult to infer what kind of information is present in the representations. We introduce here 10 probing tasks designed to capture simple linguistic features of sentences, and we use them to study embeddings generated by three different encoders trained in eight distinct ways, uncovering intriguing properties of both encoders and training methods.},
	urldate = {2024-03-29},
	booktitle = {Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Conneau, Alexis and Kruszewski, German and Lample, Guillaume and Barrault, Loïc and Baroni, Marco},
	editor = {Gurevych, Iryna and Miyao, Yusuke},
	month = jul,
	year = {2018},
	pages = {2126--2136},
	file = {Full Text PDF:/home/katja/snap/zotero-snap/common/Zotero/storage/AHBXX96I/Conneau et al. - 2018 - What you can cram into a single \$&!# vector Prob.pdf:application/pdf},
}

@inproceedings{kohn_whats_2015,
	address = {Lisbon, Portugal},
	title = {What's in an {Embedding}? {Analyzing} {Word} {Embeddings} through {Multilingual} {Evaluation}},
	shorttitle = {What's in an {Embedding}?},
	url = {https://aclanthology.org/D15-1246},
	doi = {10.18653/v1/D15-1246},
	urldate = {2024-03-29},
	booktitle = {Proceedings of the 2015 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Köhn, Arne},
	editor = {Màrquez, Lluís and Callison-Burch, Chris and Su, Jian},
	month = sep,
	year = {2015},
	pages = {2067--2073},
	file = {Full Text PDF:/home/katja/snap/zotero-snap/common/Zotero/storage/TSV9MVPQ/Köhn - 2015 - What's in an Embedding Analyzing Word Embeddings .pdf:application/pdf},
}

@inproceedings{hewitt_designing_2019,
	address = {Hong Kong, China},
	title = {Designing and {Interpreting} {Probes} with {Control} {Tasks}},
	url = {https://www.aclweb.org/anthology/D19-1275},
	doi = {10.18653/v1/D19-1275},
	abstract = {Probes, supervised models trained to predict properties (like parts-of-speech) from representations (like ELMo), have achieved high accuracy on a range of linguistic tasks. But does this mean that the representations encode linguistic structure or just that the probe has learned the linguistic task? In this paper, we propose control tasks, which associate word types with random outputs, to complement linguistic tasks. By construction, these tasks can only be learned by the probe itself. So a good probe, (one that reﬂects the representation), should be selective, achieving high linguistic task accuracy and low control task accuracy. The selectivity of a probe puts linguistic task accuracy in context with the probe’s capacity to memorize from word types. We construct control tasks for English part-of-speech tagging and dependency edge prediction, and show that popular probes on ELMo representations are not selective. We also ﬁnd that dropout, commonly used to control probe complexity, is ineffective for improving selectivity of MLPs, but that other forms of regularization are effective. Finally, we ﬁnd that while probes on the ﬁrst layer of ELMo yield slightly better part-of-speech tagging accuracy than the second, probes on the second layer are substantially more selective, which raises the question of which layer better represents parts-of-speech.},
	language = {en},
	urldate = {2024-03-29},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Hewitt, John and Liang, Percy},
	year = {2019},
	pages = {2733--2743},
	file = {Hewitt and Liang - 2019 - Designing and Interpreting Probes with Control Tas.pdf:/home/katja/snap/zotero-snap/common/Zotero/storage/X4KN5F2X/Hewitt and Liang - 2019 - Designing and Interpreting Probes with Control Tas.pdf:application/pdf},
}

@article{belinkov_probing_2022,
	title = {Probing {Classifiers}: {Promises}, {Shortcomings}, and {Advances}},
	volume = {48},
	issn = {0891-2017},
	shorttitle = {Probing {Classifiers}},
	url = {https://doi.org/10.1162/coli_a_00422},
	doi = {10.1162/coli_a_00422},
	abstract = {Probing classifiers have emerged as one of the prominent methodologies for interpreting and analyzing deep neural network models of natural language processing. The basic idea is simple—a classifier is trained to predict some linguistic property from a model’s representations—and has been used to examine a wide variety of models and properties. However, recent studies have demonstrated various methodological limitations of this approach. This squib critically reviews the probing classifiers framework, highlighting their promises, shortcomings, and advances.},
	number = {1},
	urldate = {2024-03-29},
	journal = {Computational Linguistics},
	author = {Belinkov, Yonatan},
	month = apr,
	year = {2022},
	pages = {207--219},
	file = {Full Text PDF:/home/katja/snap/zotero-snap/common/Zotero/storage/N7XMDAAU/Belinkov - 2022 - Probing Classifiers Promises, Shortcomings, and A.pdf:application/pdf;Snapshot:/home/katja/snap/zotero-snap/common/Zotero/storage/P7LC2SGH/Probing-Classifiers-Promises-Shortcomings-and.html:text/html},
}

@misc{noauthor_bertmultilingualmd_nodate,
	title = {bert/multilingual.md at master · google-research/bert},
	url = {https://github.com/google-research/bert/blob/master/multilingual.md},
	abstract = {TensorFlow code and pre-trained models for BERT. Contribute to google-research/bert development by creating an account on GitHub.},
	language = {en},
	urldate = {2024-04-06},
	journal = {GitHub},
	file = {Snapshot:/home/katja/snap/zotero-snap/common/Zotero/storage/XML7GFFI/multilingual.html:text/html},
}

@article{acs_morphosyntactic_2023,
	title = {Morphosyntactic probing of multilingual {BERT} models},
	issn = {1351-3249, 1469-8110},
	url = {https://www.cambridge.org/core/journals/natural-language-engineering/article/morphosyntactic-probing-of-multilingual-bert-models/8C0D539D3F11FB188AB73228BA7F5805},
	doi = {10.1017/S1351324923000190},
	abstract = {We introduce an extensive dataset for multilingual probing of morphological information in language models (247 tasks across 42 languages from 10 families), each consisting of a sentence with a target word and a morphological tag as the desired label, derived from the Universal Dependencies treebanks. We find that pre-trained Transformer models (mBERT and XLM-RoBERTa) learn features that attain strong performance across these tasks. We then apply two methods to locate, for each probing task, where the disambiguating information resides in the input. The first is a new perturbation method that “masks” various parts of context; the second is the classical method of Shapley values. The most intriguing finding that emerges is a strong tendency for the preceding context to hold more information relevant to the prediction than the following context.},
	language = {en},
	urldate = {2024-04-07},
	journal = {Natural Language Engineering},
	author = {Acs, Judit and Hamerlik, Endre and Schwartz, Roy and Smith, Noah A. and Kornai, Andras},
	month = may,
	year = {2023},
	keywords = {Machine Learning, Language Models, Language Resources, Morphology, Multilinguality},
	pages = {1--40},
	file = {Full Text PDF:/home/katja/snap/zotero-snap/common/Zotero/storage/TB9ZZQYB/Acs et al. - 2023 - Morphosyntactic probing of multilingual BERT model.pdf:application/pdf},
}

@inproceedings{liu_linguistic_2019,
	address = {Minneapolis, Minnesota},
	title = {Linguistic {Knowledge} and {Transferability} of {Contextual} {Representations}},
	url = {https://aclanthology.org/N19-1112},
	doi = {10.18653/v1/N19-1112},
	abstract = {Contextual word representations derived from large-scale neural language models are successful across a diverse set of NLP tasks, suggesting that they encode useful and transferable features of language. To shed light on the linguistic knowledge they capture, we study the representations produced by several recent pretrained contextualizers (variants of ELMo, the OpenAI transformer language model, and BERT) with a suite of sixteen diverse probing tasks. We find that linear models trained on top of frozen contextual representations are competitive with state-of-the-art task-specific models in many cases, but fail on tasks requiring fine-grained linguistic knowledge (e.g., conjunct identification). To investigate the transferability of contextual word representations, we quantify differences in the transferability of individual layers within contextualizers, especially between recurrent neural networks (RNNs) and transformers. For instance, higher layers of RNNs are more task-specific, while transformer layers do not exhibit the same monotonic trend. In addition, to better understand what makes contextual word representations transferable, we compare language model pretraining with eleven supervised pretraining tasks. For any given task, pretraining on a closely related task yields better performance than language model pretraining (which is better on average) when the pretraining dataset is fixed. However, language model pretraining on more data gives the best results.},
	urldate = {2024-04-08},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Liu, Nelson F. and Gardner, Matt and Belinkov, Yonatan and Peters, Matthew E. and Smith, Noah A.},
	editor = {Burstein, Jill and Doran, Christy and Solorio, Thamar},
	month = jun,
	year = {2019},
	pages = {1073--1094},
	file = {Full Text PDF:/home/katja/snap/zotero-snap/common/Zotero/storage/93IHEAYN/Liu et al. - 2019 - Linguistic Knowledge and Transferability of Contex.pdf:application/pdf},
}

@misc{alain_understanding_2018,
	title = {Understanding intermediate layers using linear classifier probes},
	url = {http://arxiv.org/abs/1610.01644},
	doi = {10.48550/arXiv.1610.01644},
	abstract = {Neural network models have a reputation for being black boxes. We propose to monitor the features at every layer of a model and measure how suitable they are for classification. We use linear classifiers, which we refer to as "probes", trained entirely independently of the model itself. This helps us better understand the roles and dynamics of the intermediate layers. We demonstrate how this can be used to develop a better intuition about models and to diagnose potential problems. We apply this technique to the popular models Inception v3 and Resnet-50. Among other things, we observe experimentally that the linear separability of features increase monotonically along the depth of the model.},
	urldate = {2024-04-08},
	publisher = {arXiv},
	author = {Alain, Guillaume and Bengio, Yoshua},
	month = nov,
	year = {2018},
	note = {arXiv:1610.01644 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/katja/snap/zotero-snap/common/Zotero/storage/YXZFEMVU/Alain and Bengio - 2018 - Understanding intermediate layers using linear cla.pdf:application/pdf;arXiv.org Snapshot:/home/katja/snap/zotero-snap/common/Zotero/storage/KI4M67DX/1610.html:text/html},
}

@inproceedings{pennington_glove_2014,
	address = {Doha, Qatar},
	title = {{GloVe}: {Global} {Vectors} for {Word} {Representation}},
	shorttitle = {{GloVe}},
	url = {https://aclanthology.org/D14-1162},
	doi = {10.3115/v1/D14-1162},
	urldate = {2024-04-09},
	booktitle = {Proceedings of the 2014 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
	editor = {Moschitti, Alessandro and Pang, Bo and Daelemans, Walter},
	month = oct,
	year = {2014},
	pages = {1532--1543},
	file = {Full Text PDF:/home/katja/snap/zotero-snap/common/Zotero/storage/ENCRZKTW/Pennington et al. - 2014 - GloVe Global Vectors for Word Representation.pdf:application/pdf},
}
