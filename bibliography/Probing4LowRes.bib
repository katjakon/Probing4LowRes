
@inproceedings{vaswani_attention_2017,
	title = {Attention is {All} you {Need}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
	abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
	urldate = {2022-08-13},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
	year = {2017},
	file = {Full Text PDF:/home/katja/snap/zotero-snap/common/Zotero/storage/W2GI4986/Vaswani et al. - 2017 - Attention is All you Need.pdf:application/pdf},
}

@inproceedings{devlin_bert_2019,
	address = {Minneapolis, Minnesota},
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {https://aclanthology.org/N19-1423},
	doi = {10.18653/v1/N19-1423},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2022-08-01},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = jun,
	year = {2019},
	pages = {4171--4186},
	file = {Full Text PDF:/home/katja/snap/zotero-snap/common/Zotero/storage/2WH2HXA2/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf},
}

@inproceedings{nivre_universal_2020,
	address = {Marseille, France},
	title = {Universal {Dependencies} v2: {An} {Evergrowing} {Multilingual} {Treebank} {Collection}},
	isbn = {979-10-95546-34-4},
	shorttitle = {Universal {Dependencies} v2},
	url = {https://aclanthology.org/2020.lrec-1.497},
	abstract = {Universal Dependencies is an open community effort to create cross-linguistically consistent treebank annotation for many languages within a dependency-based lexicalist framework. The annotation consists in a linguistically motivated word segmentation; a morphological layer comprising lemmas, universal part-of-speech tags, and standardized morphological features; and a syntactic layer focusing on syntactic relations between predicates, arguments and modifiers. In this paper, we describe version 2 of the universal guidelines (UD v2), discuss the major changes from UD v1 to UD v2, and give an overview of the currently available treebanks for 90 languages.},
	language = {English},
	urldate = {2022-07-11},
	booktitle = {Proceedings of the 12th {Language} {Resources} and {Evaluation} {Conference}},
	publisher = {European Language Resources Association},
	author = {Nivre, Joakim and de Marneffe, Marie-Catherine and Ginter, Filip and Hajič, Jan and Manning, Christopher D. and Pyysalo, Sampo and Schuster, Sebastian and Tyers, Francis and Zeman, Daniel},
	month = may,
	year = {2020},
	pages = {4034--4043},
	file = {Full Text PDF:/home/katja/snap/zotero-snap/common/Zotero/storage/NAJSYRZ4/Nivre et al. - 2020 - Universal Dependencies v2 An Evergrowing Multilin.pdf:application/pdf},
}

@misc{rogers_primer_2020,
	title = {A {Primer} in {BERTology}: {What} we know about how {BERT} works},
	shorttitle = {A {Primer} in {BERTology}},
	url = {http://arxiv.org/abs/2002.12327},
	doi = {10.48550/arXiv.2002.12327},
	abstract = {Transformer-based models have pushed state of the art in many areas of NLP, but our understanding of what is behind their success is still limited. This paper is the first survey of over 150 studies of the popular BERT model. We review the current state of knowledge about how BERT works, what kind of information it learns and how it is represented, common modifications to its training objectives and architecture, the overparameterization issue and approaches to compression. We then outline directions for future research.},
	urldate = {2023-05-31},
	publisher = {arXiv},
	author = {Rogers, Anna and Kovaleva, Olga and Rumshisky, Anna},
	month = nov,
	year = {2020},
	note = {arXiv:2002.12327 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/katja/snap/zotero-snap/common/Zotero/storage/4Z6D7IFJ/Rogers et al. - 2020 - A Primer in BERTology What we know about how BERT.pdf:application/pdf;arXiv.org Snapshot:/home/katja/snap/zotero-snap/common/Zotero/storage/BRND4ITS/2002.html:text/html},
}

@inproceedings{conneau_what_2018,
	address = {Melbourne, Australia},
	title = {What you can cram into a single \$\&!\#* vector: {Probing} sentence embeddings for linguistic properties},
	shorttitle = {What you can cram into a single \$\&!\#* vector},
	url = {https://aclanthology.org/P18-1198},
	doi = {10.18653/v1/P18-1198},
	abstract = {Although much effort has recently been devoted to training high-quality sentence embeddings, we still have a poor understanding of what they are capturing. “Downstream” tasks, often based on sentence classification, are commonly used to evaluate the quality of sentence representations. The complexity of the tasks makes it however difficult to infer what kind of information is present in the representations. We introduce here 10 probing tasks designed to capture simple linguistic features of sentences, and we use them to study embeddings generated by three different encoders trained in eight distinct ways, uncovering intriguing properties of both encoders and training methods.},
	urldate = {2024-03-29},
	booktitle = {Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Conneau, Alexis and Kruszewski, German and Lample, Guillaume and Barrault, Loïc and Baroni, Marco},
	editor = {Gurevych, Iryna and Miyao, Yusuke},
	month = jul,
	year = {2018},
	pages = {2126--2136},
	file = {Full Text PDF:/home/katja/snap/zotero-snap/common/Zotero/storage/AHBXX96I/Conneau et al. - 2018 - What you can cram into a single \$&!# vector Prob.pdf:application/pdf},
}

@inproceedings{kohn_whats_2015,
	address = {Lisbon, Portugal},
	title = {What's in an {Embedding}? {Analyzing} {Word} {Embeddings} through {Multilingual} {Evaluation}},
	shorttitle = {What's in an {Embedding}?},
	url = {https://aclanthology.org/D15-1246},
	doi = {10.18653/v1/D15-1246},
	urldate = {2024-03-29},
	booktitle = {Proceedings of the 2015 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Köhn, Arne},
	editor = {Màrquez, Lluís and Callison-Burch, Chris and Su, Jian},
	month = sep,
	year = {2015},
	pages = {2067--2073},
	file = {Full Text PDF:/home/katja/snap/zotero-snap/common/Zotero/storage/TSV9MVPQ/Köhn - 2015 - What's in an Embedding Analyzing Word Embeddings .pdf:application/pdf},
}

@inproceedings{hewitt_designing_2019,
	address = {Hong Kong, China},
	title = {Designing and {Interpreting} {Probes} with {Control} {Tasks}},
	url = {https://www.aclweb.org/anthology/D19-1275},
	doi = {10.18653/v1/D19-1275},
	abstract = {Probes, supervised models trained to predict properties (like parts-of-speech) from representations (like ELMo), have achieved high accuracy on a range of linguistic tasks. But does this mean that the representations encode linguistic structure or just that the probe has learned the linguistic task? In this paper, we propose control tasks, which associate word types with random outputs, to complement linguistic tasks. By construction, these tasks can only be learned by the probe itself. So a good probe, (one that reﬂects the representation), should be selective, achieving high linguistic task accuracy and low control task accuracy. The selectivity of a probe puts linguistic task accuracy in context with the probe’s capacity to memorize from word types. We construct control tasks for English part-of-speech tagging and dependency edge prediction, and show that popular probes on ELMo representations are not selective. We also ﬁnd that dropout, commonly used to control probe complexity, is ineffective for improving selectivity of MLPs, but that other forms of regularization are effective. Finally, we ﬁnd that while probes on the ﬁrst layer of ELMo yield slightly better part-of-speech tagging accuracy than the second, probes on the second layer are substantially more selective, which raises the question of which layer better represents parts-of-speech.},
	language = {en},
	urldate = {2024-03-29},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Hewitt, John and Liang, Percy},
	year = {2019},
	pages = {2733--2743},
	file = {Hewitt and Liang - 2019 - Designing and Interpreting Probes with Control Tas.pdf:/home/katja/snap/zotero-snap/common/Zotero/storage/X4KN5F2X/Hewitt and Liang - 2019 - Designing and Interpreting Probes with Control Tas.pdf:application/pdf},
}

@article{belinkov_probing_2022,
	title = {Probing {Classifiers}: {Promises}, {Shortcomings}, and {Advances}},
	volume = {48},
	issn = {0891-2017},
	shorttitle = {Probing {Classifiers}},
	url = {https://doi.org/10.1162/coli_a_00422},
	doi = {10.1162/coli_a_00422},
	abstract = {Probing classifiers have emerged as one of the prominent methodologies for interpreting and analyzing deep neural network models of natural language processing. The basic idea is simple—a classifier is trained to predict some linguistic property from a model’s representations—and has been used to examine a wide variety of models and properties. However, recent studies have demonstrated various methodological limitations of this approach. This squib critically reviews the probing classifiers framework, highlighting their promises, shortcomings, and advances.},
	number = {1},
	urldate = {2024-03-29},
	journal = {Computational Linguistics},
	author = {Belinkov, Yonatan},
	month = apr,
	year = {2022},
	pages = {207--219},
	file = {Full Text PDF:/home/katja/snap/zotero-snap/common/Zotero/storage/N7XMDAAU/Belinkov - 2022 - Probing Classifiers Promises, Shortcomings, and A.pdf:application/pdf;Snapshot:/home/katja/snap/zotero-snap/common/Zotero/storage/P7LC2SGH/Probing-Classifiers-Promises-Shortcomings-and.html:text/html},
}

@misc{noauthor_bertmultilingualmd_nodate,
	title = {bert/multilingual.md at master · google-research/bert},
	url = {https://github.com/google-research/bert/blob/master/multilingual.md},
	abstract = {TensorFlow code and pre-trained models for BERT. Contribute to google-research/bert development by creating an account on GitHub.},
	language = {en},
	urldate = {2024-04-06},
	journal = {GitHub},
	file = {Snapshot:/home/katja/snap/zotero-snap/common/Zotero/storage/XML7GFFI/multilingual.html:text/html},
}

@article{acs_morphosyntactic_2023,
	title = {Morphosyntactic probing of multilingual {BERT} models},
	issn = {1351-3249, 1469-8110},
	url = {https://www.cambridge.org/core/journals/natural-language-engineering/article/morphosyntactic-probing-of-multilingual-bert-models/8C0D539D3F11FB188AB73228BA7F5805},
	doi = {10.1017/S1351324923000190},
	abstract = {We introduce an extensive dataset for multilingual probing of morphological information in language models (247 tasks across 42 languages from 10 families), each consisting of a sentence with a target word and a morphological tag as the desired label, derived from the Universal Dependencies treebanks. We find that pre-trained Transformer models (mBERT and XLM-RoBERTa) learn features that attain strong performance across these tasks. We then apply two methods to locate, for each probing task, where the disambiguating information resides in the input. The first is a new perturbation method that “masks” various parts of context; the second is the classical method of Shapley values. The most intriguing finding that emerges is a strong tendency for the preceding context to hold more information relevant to the prediction than the following context.},
	language = {en},
	urldate = {2024-04-07},
	journal = {Natural Language Engineering},
	author = {Acs, Judit and Hamerlik, Endre and Schwartz, Roy and Smith, Noah A. and Kornai, Andras},
	month = may,
	year = {2023},
	keywords = {Machine Learning, Language Models, Language Resources, Morphology, Multilinguality},
	pages = {1--40},
	file = {Full Text PDF:/home/katja/snap/zotero-snap/common/Zotero/storage/TB9ZZQYB/Acs et al. - 2023 - Morphosyntactic probing of multilingual BERT model.pdf:application/pdf},
}

@inproceedings{liu_linguistic_2019,
	address = {Minneapolis, Minnesota},
	title = {Linguistic {Knowledge} and {Transferability} of {Contextual} {Representations}},
	url = {https://aclanthology.org/N19-1112},
	doi = {10.18653/v1/N19-1112},
	abstract = {Contextual word representations derived from large-scale neural language models are successful across a diverse set of NLP tasks, suggesting that they encode useful and transferable features of language. To shed light on the linguistic knowledge they capture, we study the representations produced by several recent pretrained contextualizers (variants of ELMo, the OpenAI transformer language model, and BERT) with a suite of sixteen diverse probing tasks. We find that linear models trained on top of frozen contextual representations are competitive with state-of-the-art task-specific models in many cases, but fail on tasks requiring fine-grained linguistic knowledge (e.g., conjunct identification). To investigate the transferability of contextual word representations, we quantify differences in the transferability of individual layers within contextualizers, especially between recurrent neural networks (RNNs) and transformers. For instance, higher layers of RNNs are more task-specific, while transformer layers do not exhibit the same monotonic trend. In addition, to better understand what makes contextual word representations transferable, we compare language model pretraining with eleven supervised pretraining tasks. For any given task, pretraining on a closely related task yields better performance than language model pretraining (which is better on average) when the pretraining dataset is fixed. However, language model pretraining on more data gives the best results.},
	urldate = {2024-04-08},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Liu, Nelson F. and Gardner, Matt and Belinkov, Yonatan and Peters, Matthew E. and Smith, Noah A.},
	editor = {Burstein, Jill and Doran, Christy and Solorio, Thamar},
	month = jun,
	year = {2019},
	pages = {1073--1094},
	file = {Full Text PDF:/home/katja/snap/zotero-snap/common/Zotero/storage/93IHEAYN/Liu et al. - 2019 - Linguistic Knowledge and Transferability of Contex.pdf:application/pdf},
}

@misc{alain_understanding_2018,
	title = {Understanding intermediate layers using linear classifier probes},
	url = {http://arxiv.org/abs/1610.01644},
	doi = {10.48550/arXiv.1610.01644},
	abstract = {Neural network models have a reputation for being black boxes. We propose to monitor the features at every layer of a model and measure how suitable they are for classification. We use linear classifiers, which we refer to as "probes", trained entirely independently of the model itself. This helps us better understand the roles and dynamics of the intermediate layers. We demonstrate how this can be used to develop a better intuition about models and to diagnose potential problems. We apply this technique to the popular models Inception v3 and Resnet-50. Among other things, we observe experimentally that the linear separability of features increase monotonically along the depth of the model.},
	urldate = {2024-04-08},
	publisher = {arXiv},
	author = {Alain, Guillaume and Bengio, Yoshua},
	month = nov,
	year = {2018},
	note = {arXiv:1610.01644 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/katja/snap/zotero-snap/common/Zotero/storage/YXZFEMVU/Alain and Bengio - 2018 - Understanding intermediate layers using linear cla.pdf:application/pdf;arXiv.org Snapshot:/home/katja/snap/zotero-snap/common/Zotero/storage/KI4M67DX/1610.html:text/html},
}

@inproceedings{pennington_glove_2014,
	address = {Doha, Qatar},
	title = {{GloVe}: {Global} {Vectors} for {Word} {Representation}},
	shorttitle = {{GloVe}},
	url = {https://aclanthology.org/D14-1162},
	doi = {10.3115/v1/D14-1162},
	urldate = {2024-04-09},
	booktitle = {Proceedings of the 2014 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
	editor = {Moschitti, Alessandro and Pang, Bo and Daelemans, Walter},
	month = oct,
	year = {2014},
	pages = {1532--1543},
	file = {Full Text PDF:/home/katja/snap/zotero-snap/common/Zotero/storage/ENCRZKTW/Pennington et al. - 2014 - GloVe Global Vectors for Word Representation.pdf:application/pdf},
}

@misc{noauthor_glossaryshannon_nodate,
	title = {Glossary:{Shannon} evenness index ({SEI})},
	shorttitle = {Glossary},
	url = {https://ec.europa.eu/eurostat/statistics-explained/index.php?title=Glossary:Shannon_evenness_index_(SEI)},
	abstract = {description},
	language = {en},
	urldate = {2024-04-13},
	file = {Snapshot:/home/katja/snap/zotero-snap/common/Zotero/storage/QQFZEDUQ/index.html:text/html},
}

@article{dejong_comparison_1975,
	title = {A {Comparison} of {Three} {Diversity} {Indices} {Based} on {Their} {Components} of {Richness} and {Evenness}},
	volume = {26},
	issn = {0030-1299},
	url = {https://www.jstor.org/stable/3543712},
	doi = {10.2307/3543712},
	abstract = {Species diversity was calculated using three different indices on sets of real and artificial data. Each index was analyzed to determine its relationship to the two component parts of diversity, richness and evenness. Shannon's information formula, H$^{\textrm{′}}$= CΣ p$_{\textrm{ i}}$ log2 p$_{\textrm{ i}}$, is found to be linearly related to evenness and to the log$_{\textrm{2}}$ of the number of species. Simpson's Index, D=1-∑ n( n-1)/ N( N-1) and McIntosh's Index, {\textless}tex-math{\textgreater}\$\{{\textbackslash}rm D\}{\textasciicircum}\{{\textbackslash}prime\}=\{{\textbackslash}textstyle{\textbackslash}frac\{\{{\textbackslash}rm N\}-{\textbackslash}surd {\textbackslash}overline\{{\textbackslash}Sigma \{{\textbackslash}rm n\}\_\{\{{\textbackslash}rm i\}\}\{\}{\textasciicircum}\{2\}\}\}\{\{{\textbackslash}rm N\}-{\textbackslash}surd {\textbackslash}overline\{\{{\textbackslash}rm N\}\}\}\}\${\textless}/tex-math{\textgreater}, when expressed in probits are found to be linearly related to evenness and to the log$_{\textrm{2}}$ of the number of species. Relationships between, and usefulness of, the indices are discussed. /// Рассчитено видовое разнообразие с помошью трех различных индексов по сериям реальных и моделированных данных. Каждый индекс проанализирован для определения его зависимости от двух компонент разнообразия, богатства и выровненности. Инфорационная формула ІІІэннона H$^{\textrm{′}}$= CΣ p$_{\textrm{ i}}$ log2 p$_{\textrm{ i}}$ находится в линейной зависимости от выровненности и log$_{\textrm{2}}$ от обшего количества видов. Индекс Симпсона D=1-∑ n( n-1)/ N( N-1) и индекс Макинтоша {\textless}tex-math{\textgreater}\$\{{\textbackslash}rm D\}{\textasciicircum}\{{\textbackslash}prime\}=\{{\textbackslash}textstyle{\textbackslash}frac\{\{{\textbackslash}rm N\}-{\textbackslash}surd {\textbackslash}overline\{{\textbackslash}Sigma \{{\textbackslash}rm n\}\_\{\{{\textbackslash}rm i\}\}\{\}{\textasciicircum}\{2\}\}\}\{\{{\textbackslash}rm N\}-{\textbackslash}surd {\textbackslash}overline\{\{{\textbackslash}rm N\}\}\}\}\${\textless}/tex-math{\textgreater} выраженныеі в битах, находятся в линейной зависимости от выровненности и log$_{\textrm{2}}$ от числа видов. Обсуждаются отношения между индексами и возможности их применения.},
	number = {2},
	urldate = {2024-04-13},
	journal = {Oikos},
	author = {DeJong, T. M.},
	year = {1975},
	note = {Publisher: [Nordic Society Oikos, Wiley]},
	pages = {222--227},
	file = {JSTOR Full Text PDF:/home/katja/snap/zotero-snap/common/Zotero/storage/NZP8NPSA/DeJong - 1975 - A Comparison of Three Diversity Indices Based on T.pdf:application/pdf},
}

@article{belinkov_probing_2022-1,
	title = {Probing {Classifiers}: {Promises}, {Shortcomings}, and {Advances}},
	volume = {48},
	shorttitle = {Probing {Classifiers}},
	url = {https://aclanthology.org/2022.cl-1.7},
	doi = {10.1162/coli_a_00422},
	abstract = {Probing classifiers have emerged as one of the prominent methodologies for interpreting and analyzing deep neural network models of natural language processing. The basic idea is simple—a classifier is trained to predict some linguistic property from a model's representations—and has been used to examine a wide variety of models and properties. However, recent studies have demonstrated various methodological limitations of this approach. This squib critically reviews the probing classifiers framework, highlighting their promises, shortcomings, and advances.},
	number = {1},
	urldate = {2024-04-13},
	journal = {Computational Linguistics},
	author = {Belinkov, Yonatan},
	month = mar,
	year = {2022},
	note = {Place: Cambridge, MA
Publisher: MIT Press},
	pages = {207--219},
	file = {Full Text PDF:/home/katja/snap/zotero-snap/common/Zotero/storage/D958U9EF/Belinkov - 2022 - Probing Classifiers Promises, Shortcomings, and A.pdf:application/pdf},
}

@inproceedings{ravichander_probing_2021,
	address = {Online},
	title = {Probing the {Probing} {Paradigm}: {Does} {Probing} {Accuracy} {Entail} {Task} {Relevance}?},
	shorttitle = {Probing the {Probing} {Paradigm}},
	url = {https://aclanthology.org/2021.eacl-main.295},
	doi = {10.18653/v1/2021.eacl-main.295},
	abstract = {Although neural models have achieved impressive results on several NLP benchmarks, little is understood about the mechanisms they use to perform language tasks. Thus, much recent attention has been devoted to analyzing the sentence representations learned by neural encoders, through the lens of `probing' tasks. However, to what extent was the information encoded in sentence representations, as discovered through a probe, actually used by the model to perform its task? In this work, we examine this probing paradigm through a case study in Natural Language Inference, showing that models can learn to encode linguistic properties even if they are not needed for the task on which the model was trained. We further identify that pretrained word embeddings play a considerable role in encoding these properties rather than the training task itself, highlighting the importance of careful controls when designing probing experiments. Finally, through a set of controlled synthetic tasks, we demonstrate models can encode these properties considerably above chance-level, even when distributed in the data as random noise, calling into question the interpretation of absolute claims on probing tasks.},
	urldate = {2024-04-13},
	booktitle = {Proceedings of the 16th {Conference} of the {European} {Chapter} of the {Association} for {Computational} {Linguistics}: {Main} {Volume}},
	publisher = {Association for Computational Linguistics},
	author = {Ravichander, Abhilasha and Belinkov, Yonatan and Hovy, Eduard},
	editor = {Merlo, Paola and Tiedemann, Jorg and Tsarfaty, Reut},
	month = apr,
	year = {2021},
	pages = {3363--3377},
	file = {Full Text PDF:/home/katja/snap/zotero-snap/common/Zotero/storage/SPYA8CC6/Ravichander et al. - 2021 - Probing the Probing Paradigm Does Probing Accurac.pdf:application/pdf},
}

@article{sahin_linspector_2020,
	title = {{LINSPECTOR}: {Multilingual} {Probing} {Tasks} for {Word} {Representations}},
	volume = {46},
	shorttitle = {{LINSPECTOR}},
	url = {https://aclanthology.org/2020.cl-2.4},
	doi = {10.1162/coli_a_00376},
	abstract = {Despite an ever-growing number of word representation models introduced for a large number of languages, there is a lack of a standardized technique to provide insights into what is captured by these models. Such insights would help the community to get an estimate of the downstream task performance, as well as to design more informed neural architectures, while avoiding extensive experimentation that requires substantial computational resources not all researchers have access to. A recent development in NLP is to use simple classification tasks, also called probing tasks, that test for a single linguistic feature such as part-of-speech. Existing studies mostly focus on exploring the linguistic information encoded by the continuous representations of English text. However, from a typological perspective the morphologically poor English is rather an outlier: The information encoded by the word order and function words in English is often stored on a subword, morphological level in other languages. To address this, we introduce 15 type-level probing tasks such as case marking, possession, word length, morphological tag count, and pseudoword identification for 24 languages. We present a reusable methodology for creation and evaluation of such tests in a multilingual setting, which is challenging because of a lack of resources, lower quality of tools, and differences among languages. We then present experiments on several diverse multilingual word embedding models, in which we relate the probing task performance for a diverse set of languages to a range of five classic NLP tasks: POS-tagging, dependency parsing, semantic role labeling, named entity recognition, and natural language inference. We find that a number of probing tests have significantly high positive correlation to the downstream tasks, especially for morphologically rich languages. We show that our tests can be used to explore word embeddings or black-box neural models for linguistic cues in a multilingual setting. We release the probing data sets and the evaluation suite LINSPECTOR with https://github.com/UKPLab/linspector.},
	number = {2},
	urldate = {2024-04-14},
	journal = {Computational Linguistics},
	author = {Şahin, Gözde Gül and Vania, Clara and Kuznetsov, Ilia and Gurevych, Iryna},
	month = jun,
	year = {2020},
	pages = {335--385},
	file = {Full Text PDF:/home/katja/snap/zotero-snap/common/Zotero/storage/VRVWDBYX/Şahin et al. - 2020 - LINSPECTOR Multilingual Probing Tasks for Word Re.pdf:application/pdf},
}

@inproceedings{chi_finding_2020,
	address = {Online},
	title = {Finding {Universal} {Grammatical} {Relations} in {Multilingual} {BERT}},
	url = {https://aclanthology.org/2020.acl-main.493},
	doi = {10.18653/v1/2020.acl-main.493},
	abstract = {Recent work has found evidence that Multilingual BERT (mBERT), a transformer-based multilingual masked language model, is capable of zero-shot cross-lingual transfer, suggesting that some aspects of its representations are shared cross-lingually. To better understand this overlap, we extend recent work on finding syntactic trees in neural networks' internal representations to the multilingual setting. We show that subspaces of mBERT representations recover syntactic tree distances in languages other than English, and that these subspaces are approximately shared across languages. Motivated by these results, we present an unsupervised analysis method that provides evidence mBERT learns representations of syntactic dependency labels, in the form of clusters which largely agree with the Universal Dependencies taxonomy. This evidence suggests that even without explicit supervision, multilingual masked language models learn certain linguistic universals.},
	urldate = {2024-04-14},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Chi, Ethan A. and Hewitt, John and Manning, Christopher D.},
	editor = {Jurafsky, Dan and Chai, Joyce and Schluter, Natalie and Tetreault, Joel},
	month = jul,
	year = {2020},
	pages = {5564--5577},
	file = {Full Text PDF:/home/katja/snap/zotero-snap/common/Zotero/storage/QD6H4IBA/Chi et al. - 2020 - Finding Universal Grammatical Relations in Multili.pdf:application/pdf},
}

@inproceedings{mikhailov_morph_2021,
	address = {Online},
	title = {Morph {Call}: {Probing} {Morphosyntactic} {Content} of {Multilingual} {Transformers}},
	shorttitle = {Morph {Call}},
	url = {https://aclanthology.org/2021.sigtyp-1.10},
	doi = {10.18653/v1/2021.sigtyp-1.10},
	abstract = {The outstanding performance of transformer-based language models on a great variety of NLP and NLU tasks has stimulated interest in exploration of their inner workings. Recent research has been primarily focused on higher-level and complex linguistic phenomena such as syntax, semantics, world knowledge and common-sense. The majority of the studies is anglocentric, and little remains known regarding other languages, specifically their morphosyntactic properties. To this end, our work presents Morph Call, a suite of 46 probing tasks for four Indo-European languages of different morphology: Russian, French, English and German. We propose a new type of probing tasks based on detection of guided sentence perturbations. We use a combination of neuron-, layer- and representation-level introspection techniques to analyze the morphosyntactic content of four multilingual transformers, including their understudied distilled versions. Besides, we examine how fine-tuning on POS-tagging task affects the probing performance.},
	urldate = {2024-04-14},
	booktitle = {Proceedings of the {Third} {Workshop} on {Computational} {Typology} and {Multilingual} {NLP}},
	publisher = {Association for Computational Linguistics},
	author = {Mikhailov, Vladislav and Serikov, Oleg and Artemova, Ekaterina},
	editor = {Vylomova, Ekaterina and Salesky, Elizabeth and Mielke, Sabrina and Lapesa, Gabriella and Kumar, Ritesh and Hammarström, Harald and Vulić, Ivan and Korhonen, Anna and Reichart, Roi and Ponti, Edoardo Maria and Cotterell, Ryan},
	month = jun,
	year = {2021},
	pages = {97--121},
	file = {Full Text PDF:/home/katja/snap/zotero-snap/common/Zotero/storage/DZBR3553/Mikhailov et al. - 2021 - Morph Call Probing Morphosyntactic Content of Mul.pdf:application/pdf},
}

@inproceedings{ravishankar_multilingual_2019,
	address = {Turku, Finland},
	title = {Multilingual {Probing} of {Deep} {Pre}-{Trained} {Contextual} {Encoders}},
	url = {https://aclanthology.org/W19-6205},
	abstract = {Encoders that generate representations based on context have, in recent years, benefited from adaptations that allow for pre-training on large text corpora. Earlier work on evaluating fixed-length sentence representations has included the use of `probing' tasks, that use diagnostic classifiers to attempt to quantify the extent to which these encoders capture specific linguistic phenomena. The principle of probing has also resulted in extended evaluations that include relatively newer word-level pre-trained encoders. We build on probing tasks established in the literature and comprehensively evaluate and analyse – from a typological perspective amongst others – multilingual variants of existing encoders on probing datasets constructed for 6 non-English languages. Specifically, we probe each layer of a multiple monolingual RNN-based ELMo models, the transformer-based BERT's cased and uncased multilingual variants, and a variant of BERT that uses a cross-lingual modelling scheme (XLM).},
	urldate = {2024-04-14},
	booktitle = {Proceedings of the {First} {NLPL} {Workshop} on {Deep} {Learning} for {Natural} {Language} {Processing}},
	publisher = {Linköping University Electronic Press},
	author = {Ravishankar, Vinit and Gökırmak, Memduh and Øvrelid, Lilja and Velldal, Erik},
	editor = {Nivre, Joakim and Derczynski, Leon and Ginter, Filip and Lindi, Bjørn and Oepen, Stephan and Søgaard, Anders and Tidemann, Jörg},
	month = sep,
	year = {2019},
	pages = {37--47},
	file = {Full Text PDF:/home/katja/snap/zotero-snap/common/Zotero/storage/GAA6BJ8M/Ravishankar et al. - 2019 - Multilingual Probing of Deep Pre-Trained Contextua.pdf:application/pdf},
}
